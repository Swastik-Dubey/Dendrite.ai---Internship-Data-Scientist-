{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18518eda-3f8c-4ac8-90de-1899f27cf1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# Function to load JSON data from an RTF file\n",
    "def load_json_from_rtf(file_path):\n",
    "    # Read the RTF file and extract plain text\n",
    "    with open(file_path, 'r') as file:\n",
    "        rtf_content = file.read()\n",
    "\n",
    "    plain_text = rtf_to_text(rtf_content)\n",
    "\n",
    "    # Load the extracted text as JSON data\n",
    "    json_data = json.loads(plain_text)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# Function to load a CSV file as a Pandas DataFrame\n",
    "def load_csv_as_dataframe(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to create a feature handling pipeline\n",
    "def create_feature_handling_pipeline(feature_handling):\n",
    "    steps = []\n",
    "    \n",
    "    # Iterate through each feature to handle missing values\n",
    "    for feature, details in feature_handling.items():\n",
    "        feature_details = details.get('feature_details')\n",
    "        if feature_details:\n",
    "            impute_with = feature_details.get('impute_with')\n",
    "            impute_value = feature_details.get('impute_value')\n",
    "            if impute_with and impute_value:\n",
    "                if impute_with == 'Impute':\n",
    "                    if impute_value == 'Average of values':\n",
    "                        imputer = SimpleImputer(strategy='mean')\n",
    "                        steps.append((f'imputer_{feature}', imputer))\n",
    "                    elif impute_value == 'custom':\n",
    "                        imputer = SimpleImputer(strategy='constant', fill_value=feature_details['custom_value'])\n",
    "                        steps.append((f'imputer_{feature}', imputer))\n",
    "                    # Add more conditions for other imputation methods\n",
    "    \n",
    "    if not steps:\n",
    "        # If no steps were appended, add a passthrough step\n",
    "        steps.append(('passthrough', 'passthrough'))\n",
    "    \n",
    "    feature_handling_pipeline = Pipeline(steps)\n",
    "    return feature_handling_pipeline\n",
    "\n",
    "\n",
    "# Function to create the feature reduction pipeline\n",
    "def create_feature_reduction_pipeline(json_data, df):\n",
    "    # Label encoding for object type columns\n",
    "    for feature in df.columns:\n",
    "        if df[feature].dtype == 'object':\n",
    "            label_encoder = LabelEncoder()\n",
    "            df[feature] = label_encoder.fit_transform(df[feature])\n",
    "\n",
    "    feature_reduction = json_data['design_state_data']['feature_reduction']\n",
    "    method = feature_reduction.get('feature_reduction_method')\n",
    "    target = json_data['design_state_data']['target']['target']\n",
    "    X = df.drop(columns=[target]) if target in df.columns else df.copy()\n",
    "\n",
    "    steps = []\n",
    "\n",
    "    if method == 'No Reduction':\n",
    "        return None, X, df[target]  # No reduction to be performed\n",
    "\n",
    "    if method == 'Corr with Target':\n",
    "        if target in df.columns:\n",
    "            corr_values = df.corr()[target].sort_values(ascending=False)\n",
    "            num_features_to_select = 5\n",
    "            selected_features = corr_values.index[1:num_features_to_select + 1]\n",
    "            if target in df.columns:\n",
    "                steps.append(('feature_selection', SelectKBest(score_func=f_regression, k=num_features_to_select)))\n",
    "\n",
    "    if method == 'Tree-based':\n",
    "        if target in df.columns:\n",
    "            model = RandomForestRegressor()\n",
    "            model.fit(X, df[target])\n",
    "            feature_importance = model.feature_importances_\n",
    "            num_features_to_select = 5\n",
    "            steps.append(('feature_selection', SelectKBest(k=num_features_to_select)))\n",
    "\n",
    "    if method == 'PCA':\n",
    "        if len(X.columns) > 0:  # Check if there are columns to perform PCA\n",
    "            n_components = min(3, X.shape[1])  # Adjust the number of components if needed\n",
    "            pca = PCA(n_components=n_components)\n",
    "            steps.append(('pca', pca))\n",
    "\n",
    "    feature_reduction_pipeline = Pipeline(steps)\n",
    "    return feature_reduction_pipeline, X, df[target]\n",
    "\n",
    "\n",
    "\n",
    "# Function to create the model fitting pipeline\n",
    "def create_model_fitting_pipeline(json_data, df, X, y):\n",
    "    # Extract the split parameters\n",
    "    train_params = json_data['design_state_data']['train']\n",
    "    train_ratio = train_params['train_ratio']\n",
    "    random_seed = train_params['random_seed']\n",
    "    \n",
    "    # Extracting the target prediction type and the algorithms section\n",
    "    prediction_type = json_data['design_state_data']['target']['prediction_type']\n",
    "    algorithms = json_data['design_state_data']['algorithms']\n",
    "\n",
    "    # Define model parameters based on the algorithm configuration in the JSON\n",
    "    model_parameters = {\n",
    "        \"RandomForestClassifier\": algorithms.get(\"RandomForestClassifier\", {}),\n",
    "        \"RandomForestRegressor\": algorithms.get(\"RandomForestRegressor\", {}),\n",
    "        \"GBTClassifier\": algorithms.get(\"GBTClassifier\", {}),\n",
    "        \"GBTRegressor\": algorithms.get(\"GBTRegressor\", {}),\n",
    "        \"LinearRegression\": algorithms.get(\"LinearRegression\", {}),\n",
    "        \"LogisticRegression\": algorithms.get(\"LogisticRegression\", {}),\n",
    "        \"RidgeRegression\": algorithms.get(\"RidgeRegression\", {}),\n",
    "        \"LassoRegression\": algorithms.get(\"LassoRegression\", {}),\n",
    "        \"ElasticNetRegression\": algorithms.get(\"ElasticNetRegression\", {}),\n",
    "        \"xg_boost\": algorithms.get(\"xg_boost\", {}),\n",
    "        \"DecisionTreeRegressor\": algorithms.get(\"DecisionTreeRegressor\", {}),\n",
    "        \"DecisionTreeClassifier\": algorithms.get(\"DecisionTreeClassifier\", {}),\n",
    "        \"SVM\": algorithms.get(\"SVM\", {}),\n",
    "        \"SGD\": algorithms.get(\"SGD\", {}),\n",
    "        \"KNN\": algorithms.get(\"KNN\", {}),\n",
    "        \"extra_random_trees\": algorithms.get(\"extra_random_trees\", {}),\n",
    "        \"neural_network\": algorithms.get(\"neural_network\", {})\n",
    "    }\n",
    "\n",
    "    # Predefined model objects for regression and classification\n",
    "    model_objects = {\n",
    "        \"Regression\": {\n",
    "            \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "            \"GBTRegressor\": XGBRegressor(),\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"LassoRegression\": Lasso(),\n",
    "            \"RidgeRegression\": Ridge(),\n",
    "            \"ElasticNetRegression\": ElasticNet(),\n",
    "            \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "            \"SVR\": SVR(),\n",
    "            \"SGD\": SGDRegressor(),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"extra_random_trees\": RandomForestRegressor(),  # Change this with specific model object if available\n",
    "            \"neural_network\": MLPRegressor()\n",
    "        },\n",
    "        \"Classification\": {\n",
    "            \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "            \"GBTClassifier\": XGBClassifier(),\n",
    "            \"LogisticRegression\": LogisticRegression(),\n",
    "            \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "            \"SVM\": SVC(),\n",
    "            \"SGD\": SGDClassifier(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"neural_network\": MLPClassifier()\n",
    "        }\n",
    "    }\n",
    "    # Selecting model objects based on the prediction type\n",
    "    selected_models = model_objects[prediction_type]\n",
    "    selected_models_to_tune = []\n",
    "    \n",
    "    for model_name, model_instance in selected_models.items():\n",
    "        # Ensure 'is_selected' is set to True for the model in the JSON\n",
    "        if model_name in model_parameters and model_parameters[model_name].get(\"is_selected\", False):\n",
    "            model_params = model_parameters[model_name]\n",
    "\n",
    "            # Create the model instance based on the model_name\n",
    "            model_instance = selected_models[model_name]  # Ensure this line is used\n",
    "\n",
    "            # Split the data for training and testing\n",
    "            if train_ratio == 0:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "            else:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_ratio, random_state=random_seed)\n",
    "\n",
    "            # Add selected models to tune with GridSearchCV\n",
    "            selected_models_to_tune.append((model_name, model_instance, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "     # Define default parameter grids for hyperparameter tuning\n",
    "    default_param_grids = {\n",
    "        'RandomForestRegressor': {\n",
    "            'n_estimators': [100, 300, 500],\n",
    "            'max_depth': [None, 5, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "            # Add other RandomForestRegressor hyperparameters here\n",
    "        },\n",
    "        'GBTRegressor': {\n",
    "            'n_estimators': [100, 300, 500],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.1, 0.01]\n",
    "        # Add other XGBRegressor hyperparameters here\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'fit_intercept': [True, False],\n",
    "        'normalize': [True, False]\n",
    "        # Add other LinearRegression hyperparameters here\n",
    "    },\n",
    "    'LassoRegression': {\n",
    "        'alpha': [0.1, 1.0, 10.0]\n",
    "        # Add other LassoRegression hyperparameters here\n",
    "    },\n",
    "    'RidgeRegression': {\n",
    "        'alpha': [0.1, 1.0, 10.0]\n",
    "        # Add other RidgeRegression hyperparameters here\n",
    "    },\n",
    "    'ElasticNetRegression': {\n",
    "        'alpha': [0.1, 1.0, 10.0],\n",
    "        'l1_ratio': [0.1, 0.5, 0.9]\n",
    "        # Add other ElasticNetRegression hyperparameters here\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [None, 5, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "        # Add other DecisionTreeRegressor hyperparameters here\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "        # Add other SVR hyperparameters here\n",
    "    },\n",
    "    'SGD': {\n",
    "        'loss': ['squared_loss', 'huber'],\n",
    "        'alpha': [0.0001, 0.001, 0.01]\n",
    "        # Add other SGDRegressor hyperparameters here\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 10],\n",
    "        'weights': ['uniform', 'distance']\n",
    "        # Add other KNeighborsRegressor hyperparameters here\n",
    "    },\n",
    "    'extra_random_trees': {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'max_depth': [None, 5, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "        # Add other RandomForestRegressor (Extra Trees) hyperparameters here\n",
    "    },\n",
    "    'neural_network': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'activation': ['relu', 'tanh']\n",
    "        # Add other MLPRegressor (Neural Network) hyperparameters here\n",
    "        }\n",
    "    }\n",
    "    tuned_models = []\n",
    "    \n",
    "    for model_name, model_instance, X_train, y_train, X_test, y_test in selected_models_to_tune:\n",
    "        model_name = type(model_instance).__name__\n",
    "        if model_name in default_param_grids:\n",
    "            param_grid = default_param_grids[model_name]\n",
    "\n",
    "            # Use GridSearchCV to tune the models\n",
    "            grid_search = GridSearchCV(model_instance, param_grid=param_grid, cv=5)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Collect the tuned model information\n",
    "            tuned_models.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"best_estimator\": grid_search.best_estimator_,\n",
    "                \"best_params\": grid_search.best_params_,\n",
    "                \"best_score\": grid_search.best_score_,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_test\": y_test\n",
    "            })\n",
    "\n",
    "    return tuned_models\n",
    "    \n",
    "# Function to calculate model metrics\n",
    "def calculate_model_metrics(tuned_models, prediction_type, X_test, y_test):\n",
    "    for tuned_model in tuned_models:\n",
    "        model_name = tuned_model[\"model_name\"]\n",
    "        best_estimator = tuned_model[\"best_estimator\"]\n",
    "\n",
    "        if prediction_type == \"Regression\":\n",
    "            test_predictions = best_estimator.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, test_predictions)\n",
    "            r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "            print(f\"Metrics for {model_name}:\")\n",
    "            print(f\"MSE: {mse}\")\n",
    "            print(f\"R-squared: {r2}\")\n",
    "        elif prediction_type == \"Classification\":\n",
    "            test_predictions = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, test_predictions)\n",
    "            f1 = f1_score(y_test, test_predictions)\n",
    "\n",
    "            print(f\"Metrics for {model_name}:\")\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            print(f\"F1 Score: {f1}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03f28b97-d73c-486c-bee0-8b01cb3e5033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swast\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for RandomForestRegressor:\n",
      "MSE: 0.03612550333192687\n",
      "R-squared: 0.9247542109312084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_pipeline(json_file, csv_file):\n",
    "    # Load data from the provided files\n",
    "    json_data = load_json_from_rtf(json_file)  # Load and extract JSON data from the RTF file\n",
    "    df = load_csv_as_dataframe(csv_file)  # Load the CSV file as a DataFrame\n",
    "\n",
    "    pipeline_results = []  # Initialize an empty list to store model results\n",
    "\n",
    "    # Data Preprocessing Pipeline\n",
    "    feature_handling_pipeline = create_feature_handling_pipeline(json_data['design_state_data']['feature_handling'])  # Creating a data preprocessing pipeline\n",
    "    X_features_handled = feature_handling_pipeline.fit_transform(df)  # Applying the preprocessing to the DataFrame\n",
    "    target = json_data['design_state_data']['target']['target']  # Extracting the target variable for model fitting\n",
    "\n",
    "    # Feature Reduction Pipeline\n",
    "    feature_reduction_pipeline, X, y = create_feature_reduction_pipeline(json_data, df)  # Creating a feature reduction pipeline\n",
    "    X_features_reduced = feature_reduction_pipeline.fit_transform(X_features_handled, y)  # Applying feature reduction to preprocessed data\n",
    "\n",
    "    # Model Fitting Pipeline\n",
    "    tuned_models = create_model_fitting_pipeline(json_data, df, X, y)  # Creating and fitting models to the data\n",
    "\n",
    "    for tuned_model in tuned_models:\n",
    "        model = tuned_model[\"best_estimator\"]  # Extracting the best fitted model\n",
    "        try:\n",
    "            pipeline_results.append(model)  # Storing the best models in a list\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fitting the model: {e}\")  # Handling exceptions during model fitting\n",
    "\n",
    "    # Extracting the target prediction type\n",
    "    prediction_type = json_data['design_state_data']['target']['prediction_type']  # Extracting the type of prediction (Regression or Classification)\n",
    "\n",
    "    # Assuming you have these variables available: tuned_models, prediction_type, X_test, y_test\n",
    "    X_test = tuned_model[\"X_test\"]  # Obtaining the test features for model evaluation\n",
    "    y_test = tuned_model[\"y_test\"]  # Obtaining the test target values for model evaluation\n",
    "\n",
    "    calculate_model_metrics(tuned_models, prediction_type, X_test, y_test)  # Calculate and print model evaluation metrics\n",
    "    return tuned_models  # Returning the list of tuned models\n",
    "\n",
    "# Example usage:\n",
    "pipeline_results = create_pipeline('algoparams_from_ui.json.rtf', 'iris_modified.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df394a4-82e1-4247-ab32-1230038ae79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
